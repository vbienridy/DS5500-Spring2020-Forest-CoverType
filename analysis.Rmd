---
title: "analysis notebook"
output:
  html_document:
    df_print: paged
---

```{r}
# This R notebook is for feature engineering and modeling analysis
# Date: 02/20/2020

# set working directory
setwd("D:/NEU/DS 5500/code")
set.seed(5500)
```

```{r}
# load data
library(data.table)
library(readr)
train <- fread("data/train.csv")
test <- fread("data/test.csv")
test_labeled <- fread("data/test/test.csv") # test set with true class label
# names(train)
# names(test)
```

```{r}
# variable renaming for convenience
names(train)[5:11] <- c("HDTH","VDTH","HDTR","H9am","Hnoon","H3pm","HDTFP")  
names(test)[5:11] <- c("HDTH","VDTH","HDTR","H9am","Hnoon","H3pm","HDTFP")
```

```{r}
# data imputation for Hillshade (missing value is recorded as 0 in dataset) 
# find number of zero Hillshade values in the training set
sum(train$H9am == 0) # 1
sum(train$H3pm == 0) # 88
sum(train$Hnoon == 0) # 0
```

```{r}
# imputation with linear regression
mistrain <- subset(train, train$H3pm==0) 
whotrain <- train[-mistrain$Id, ]
lr <- lm(H3pm~., data=whotrain[,-"Id"])
mistrain$H3pm <- predict(lr, newdata=mistrain[, -c("Id","H3pm")])
train <- rbind.data.frame(mistrain, whotrain)
write.csv(train,"data/output/train_H3pm_lr.csv", row.names=FALSE) # save to local csv file
```

```{r}
# imputation with random forest model
library(randomForest)
mistrain <- subset(train, train$H3pm==0)
whotrain <- train[-mistrain$Id, ]
rf <- randomForest(H3pm~., data=whotrain[, -"Id"], type="regression", ntree=100)
varImpPlot(rf) # most important variables are Aspect, H9am, Hnoon, Slope
mistrain$H3pm <- predict(rf, newdata=mistrain[, -c("Id","H3pm")])

train <- rbind.data.frame(mistrain, whotrain)
write.csv(train, "data/output/train_H3pm_randomForest.csv", row.names=FALSE) # save to local csv file
```

```{r}
## imputation with xgboost model
library(xgboost)
library(Matrix)
train_xgb <- train[, c(1, 3, 4, 8, 9, 10)]
mistrain <- subset(train_xgb , train_xgb$H3pm==0)
whotrain <- train_xgb[-mistrain$Id, ]
sparse_matrix_H3pm_train <- sparse.model.matrix(~.-1, data=whotrain[, -c("Id","H3pm")])
sparse_matrix_H3pm_test <- sparse.model.matrix(~.-1, data=mistrain[, -c("Id","H3pm")])
bst <- xgboost(data=sparse_matrix_H3pm_train, label=whotrain$H3pm, max.depth=8, max_delta_step=6, colsample_bytree=0.9, colsample_bylevel=0.8, min_child_weight=4, eta=0.05, gamma=2, subsampe=0.95, nthread=4, nround=1200, objective="reg:linear")
mistrain$H3pm <- predict(bst, sparse_matrix_H3pm_test)

train <- rbind.data.frame(mistrain, whotrain)
write.csv(train, "data/output/train_H3pm_xgboost.csv", row.names=FALSE) # save to local csv file
```

```{r}
# Or use only Aspect, Slope, H9am, Hnoon features to do regression
# After data imputation, the Ids may be reordered
# check whether there are any records with H3pm=0
sum(train$H3pm == 0)
```

```{r}
# feature engineering
# linear regression for VDTH and HDTH, VDTH as target variable
library(modelr)
fit1 <- lm(VDTH ~ HDTH, data=train)
fit2 <- lm(VDTH ~ HDTH-1, data=train)
coef(fit1)
coef(fit2)
```

```{r}
# create new variables associated with vertical distances
train$Highwater = train$VDTH < 0
test$Highwater = test$VDTH < 0

train$EVDtH = train$Elevation-train$VDTH
test$EVDtH = test$Elevation-test$VDTH

train$EHDtH = train$Elevation-train$HDTH*0.2
test$EHDtH = test$Elevation-test$HDTH*0.2

# create new variables associated with horizontal distances
train$DTH = (train$HDTH**2+train$VDTH**2)**0.5
test$DTH = (test$HDTH**2+test$VDTH**2)**0.5

train$tan = (train$VDTH+0.000000000001)/(train$HDTH+0.000000000001)
test$tan = (test$VDTH+0.000000000001)/(test$HDTH+0.000000000001)

# create new variables associated with distances to hydrology
train$HF1 = train$HDTH+train$HDTFP
test$HF1 = test$HDTH+test$HDTFP

train$HF2 = abs(train$HDTH-train$HDTFP)
test$HF2 = abs(test$HDTH-test$HDTFP)

train$HF3 = (train$HDTH+0.000000000001)/(train$HDTFP+0.000000000001)
test$HF3 = (test$HDTH+0.000000000001)/(test$HDTFP+0.000000000001)

train$HR1 = train$HDTH+train$HDTR
test$HR1 = test$HDTH+test$HDTR

train$HR2 = abs(train$HDTH-train$HDTR)
test$HR2 = abs(test$HDTH-test$HDTR)

train$HR3 = (train$HDTH+0.000000000001)/(train$HDTR+0.000000000001)
test$HR3 = (test$HDTH+0.000000000001)/(test$HDTR+0.000000000001)

train$FR1 = train$HDTFP+train$HDTR
test$FR1 = test$HDTFP+test$HDTR

train$FR2 = abs(train$HDTFP-train$HDTR)
test$FR2 = abs(test$HDTFP-test$HDTR)

train$FR3 = (train$HDTFP+0.000000000001)/(train$HDTR+0.000000000001)
test$FR3 = (test$HDTFP+0.000000000001)/(test$HDTR+0.000000000001)
write.csv(train, "data/output/train_processed.csv", row.names=FALSE) # write_csv from readr
write.csv(test, "data/output/test_processed.csv", row.names=FALSE) # fwrite from data.table 
```

```{r}
# model training
# remove Id column and Cover_Type column
train_data <- train[, -"Id"]
test_data <- test[, -"Id"] # test_data <- test[, -"Id"]

# turn train and test dataset into dgCMatrix form
sparse_matrix_train <- sparse.model.matrix(Cover_Type~.-1, data=train_data)
sparse_matrix_test <- sparse.model.matrix(~.-1, data=test_data)
colnames(sparse_matrix_train)
colnames(sparse_matrix_test)
output_vector <- as.numeric(train_data$Cover_Type)-1
```

```{r}
# train random forest model
rf <- randomForest(as.factor(Cover_Type)~., data=train_data, type="class", ntree=200)
test_rf <- test_data[,]
test_rf$Cover_Type <- predict(rf, newdata=test_data) # test_data[,-"Id"]?
ftable(test_rf$Cover_Type)
write.csv(test_rf[,c("Id","Cover_Type")], "data/output/randomForest.csv", row.names=FALSE)

mean(test_labeled$Cover_Type == test_rf$Cover_Type)
```

```{r}
# train xgboost model
bst <- xgboost(data=sparse_matrix_train, label=output_vector, max.depth=25, colsample_bytree=1, eta=0.7, gamma=1, subsampe=1, nthread=2, nround=40, num_class=7, objective="multi:softmax")
test_xgb <- test_data[,]
test_xgb$Cover_Type <- predict(bst, sparse_matrix_test) + 1
ftable(test_xgb$Cover_Type)
# test_xgb$Id <- test$Id
write.csv(test_xgb[, c("Id","Cover_Type")], "data/output/xgboost.csv", row.names=FALSE)

importance <- xgb.importance(feature_names=sparse_matrix_train@Dimnames[[2]], model=bst)
head(importance) # feature importance table

mean(test_labeled$Cover_Type == test_xgb$Cover_Type) # accuracy
```

```{r}
# train extremely randomized trees model
options(java.parameters = "-Xmx4g")
library(extraTrees)
et = extraTrees(train_data[, -"Cover_Type"], train_data$CoverType)
test_et <- test_data[,]
test_et$Cover_Type <- predict(et, test_data) # probability=TRUE produces probabilities
ftable(test_et$Cover_Type)
write.csv(test_et[, c("Id","Cover_Type")], "data/output/extraTree.csv", row.names=FALSE)

mean(test_labeled$Cover_Type == test_et$Cover_Type)

write.csv(train, "data/output/train.csv", row.names=FALSE) # check file names
write.csv(test, "data/output/test.csv", row.names=FALSE)
# Or use python sklearn ExtraTreesClassifier method to train
# execute extratree.ipynb and load the result csv file
test_extra <- fread("data/output/extratree.csv")
ftable(test_extra$Cover_Type)

mean(test_labeled$Cover_Type == test_extra$Cover_Type) # accuracy
```

```{r}
# compute accuracy for each class
get_accuracy_for_label <- function(test_clf, test_labeled) {
  for (label in 1:7) {
    acc <- sum((test_clf$Cover_Type == label) & (test_clf$Cover_Type == test_labeled$Cover_Type))
    total <- sum(test_labeled$Cover_Type == label)
    print(sprintf("The accuracy for class label = %d is %.4f", label, acc/total))
  }
} 

get_accuracy_for_label(test_xgb, test_labeled)

# compute confusion matrix
library(caret)
confusionMatrix(as.factor(test_xgb$Cover_Type), as.factor(test_labeled$Cover_Type))
```

```{r}
# model plots
# cross validation
# train weak classifiers
# combine weak classifiers
# model performance evaluation and error analysis
```

```{r}
# train second classifier
train_H3pm <- fread("data/output/train_processed.csv") # H3pm imputed with correlated variables by xgboost
# train_processed.csv ?

proportions <- c(1866, 2160, 328, 17, 225, 207, 242)
samples <- list()
for (i in 1:7) {
  set.seed(2*i + 1)
  samples[[i]] <- subset(train_H3pm, train_H3pm$Cover_Type==i)
  index <- sample(1:2160, proportions[i])
  samples[[i]] <- samples[[i]][index, ]
}

train_resampled <- data.frame()
for (i in 1:7) {
  train_resampled <- rbind.data.frame(train_resampled, samples[[i]])
}

names(train_resampled)

sum(train_resampled$H9am == 0)
sum(train_resampled$H3pm == 0)
sum(train_resampled$Hnoon == 0)

write.csv(train_resampled, "data/output/train_resampled.csv", row.names=FALSE)
```

```{r}
# train third classifier
test3<-fread("data/output/strata/pred3extratree.csv")
test4<-fread("data/output/strata/pred4extratree.csv")
test7<-fread("data/output/strata/pred7extratree.csv")
test8<-fread("data/output/strata/pred8extratree.csv")
test9<-fread("data/output/strata/pred9extratree.csv")
test11<-fread("data/output/strata/pred11extratree.csv")
test12<-fread("data/output/strata/pred12extratree.csv")
test14<-fread("data/output/strata/pred13extratree.csv")
test13<-fread("data/output/strata/pred14extratree.csv")
test15<-fread("data/output/strata/pred15extratree.csv")
test17<-fread("data/output/strata/pred17extratree.csv")
test18<-fread("data/output/strata/pred18extratree.csv")
test19<-fread("data/output/strata/pred19extratree.csv")
test23<-fread("data/output/strata/pred23extratree.csv")
test25<-fread("data/output/strata/pred25extratree.csv")
test26<-fread("data/output/strata/pred26extratree.csv")
test27<-fread("data/output/strata/pred27extratree.csv")
testout<-fread("data/output/strata/predoutextratree.csv")

# combine all predictions obtain from stratification
tree1000strata <- rbind.data.frame(test3, test4, test7, test8, test9, test11, test12, test13, test14, test15, test17, test18, test19, test23, test25, test26, test27, testout)

# sort by id column ?


write.csv(tree1000strata, "data/output/extratree1000strata.csv", row.names=FALSE)

mean(test_labeled$Cover_Type == tree1000strata$Cover_Type)

get_accuracy_for_label(tree1000strata, test_labeled)

confusionMatrix(as.factor(tree1000strata$Cover_Type), as.factor(test_labeled$Cover_Type))
```

```{r}
# combine classifiers to get final prediction

# first classifier
tree1000 <- fread("data/output/extratree1000.csv") # 1000 extratrees

# second classifier
tree1000sample <- fread("data/output/extratree1000sample.csv") # 1000 extratrees, training set resampled according to results from first classifier

# third classifier
tree1000strata <- fread("data/output/extratree1000strata.csv") # 1000 extratrees, dataset stratified by soil type and wilderness area 

# function to find the mode in a vector
get_mode <- function(x) {
  unique_x <- unique(x)
  return(unique_x[which.max(tabulate(match(x, unique_x)))])
} 

# combine classifier by voting
final_prediction <- data.frame(tree1000$Id, apply(data.frame(tree1000$Cover_Type, tree1000sample$Cover_Type, tree1000strata$Cover_Type, rep(2, 565892), rep(2, 565892)), 1, get_mode))
names(final_prediction) <- c("Id", "Cover_Type")
mean(final_prediction$Cover_Type == test_labeled$Cover_Type) # total accuracy

get_accuracy_for_label(final_prediction, test_labeled)

write.csv(final_prediction,"data/output/final_prediction.csv", row.names=FALSE)

```

```{r}
# comparison between first classifier and final classifier
confusionMatrix(as.factor(tree1000$Cover_Type), as.factor(test_labeled$Cover_Type))

confusionMatrix(as.factor(final_prediction$Cover_Type), as.factor(test_labeled$Cover_Type))

```